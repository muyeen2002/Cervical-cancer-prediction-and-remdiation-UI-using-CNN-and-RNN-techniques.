{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JRSDNNtLYqsS"
      },
      "outputs": [],
      "source": [
        "# Importing Necessary Libraries\n",
        "import cv2\n",
        "import os\n",
        "import shutil\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8wmxLk4RZTx3"
      },
      "outputs": [],
      "source": [
        "def image_segmentation(original_img):\n",
        "\n",
        "#     padded_image=cv2.copyMakeBorder(original_img,1,1,1,1,cv2.BORDER_CONSTANT,value=(0,0,0))\n",
        "    J=cv2.bilateralFilter(original_img,9,75,75)\n",
        "\n",
        "#     a=np.array(J)\n",
        "#     m,n,p=a.shape\n",
        "#     for r in range(1,m-1):\n",
        "#         for c in range(1,n-1):\n",
        "#             s=J[r,c]\n",
        "#             n1=J[r+1,c]\n",
        "#             n2=J[r-1,c]\n",
        "#             n3=J[r,c+1]\n",
        "#             n4=J[r,c-1]\n",
        "#             n5=J[r-1,c-1]\n",
        "#             n6=J[r+1,c+1]\n",
        "#             n7=J[r+1,c-1]\n",
        "#             n8=J[r-1,c+1]\n",
        "\n",
        "#             s1=s-n1\n",
        "#             s2=s-n2\n",
        "#             s3=s-n3\n",
        "#             s4=s-n4\n",
        "#             s5=s-n5\n",
        "#             s6=s-n6\n",
        "#             s7=s-n7\n",
        "#             s8=s-n8\n",
        "#             threshold=150\n",
        "#             if (all(s1<threshold) and all(s2<threshold) and all(s3<threshold) and all(s4<threshold) and all(s5<threshold) and all(s6<threshold) and all(s7<threshold) and all(s8<threshold)):\n",
        "#                 J[r,c]=0;\n",
        "#             else:\n",
        "#                 J[r,c]=s;\n",
        "\n",
        "#     ret, final_img = cv2.threshold(J, 110, 255, cv2.THRESH_BINARY_INV)\n",
        "    return J;\n",
        "# Function for Formatting Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wcr_VXVrZpDt",
        "outputId": "3454ce0f-484c-4ce9-d72d-62fe1c52031f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.6.12)\n",
            "Requirement already satisfied: six>=1.10 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kaggle) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: requests in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kaggle) (4.66.2)\n",
            "Requirement already satisfied: python-slugify in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kaggle) (2.2.1)\n",
            "Requirement already satisfied: bleach in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->kaggle) (3.7)\n",
            "Requirement already satisfied: colorama in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->kaggle) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1ermQ4QaHQu",
        "outputId": "8d3dd772-31d2-4e77-e20d-2d901946d16a"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m~/.kaggle/kaggle.json\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[1;32m----> 4\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[0;32m      5\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpload kaggle.json here\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m   files\u001b[38;5;241m.\u001b[39mupload()\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "! pip install -q kaggle\n",
        "import os\n",
        "if not os.path.isfile(os.path.expanduser('~/.kaggle/kaggle.json')):\n",
        "  from google.colab import files\n",
        "  print(\"Upload kaggle.json here\")\n",
        "  files.upload()\n",
        "\n",
        "if not os.path.isfile('IMDB Dataset.csv'):\n",
        "  !mkdir ~/.kaggle\n",
        "  !mv ./kaggle.json ~/.kaggle/\n",
        "  !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "  dataset_name = 'prahladmehandiratta/cervical-cancer-largest-dataset-sipakmed'\n",
        "  zip_name = dataset_name.split('/')[-1]\n",
        "\n",
        "  !kaggle datasets download -d {dataset_name}\n",
        "  !unzip -q ./{zip_name}.zip -d ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3RP69ZylT2v",
        "outputId": "061df159-97af-4b48-98b9-b5bfe0a570e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted contents: ['im_Metaplastic', 'im_Parabasal', 'im_Koilocytotic', 'im_Superficial-Intermediate', 'im_Dyskeratotic']\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to the zip file\n",
        "zip_file_path = '/content/cervical-cancer-largest-dataset-sipakmed.zip'  # Replace with the actual path to your zip file\n",
        "\n",
        "# Directory to extract the contents\n",
        "extracted_folder_path = '/content/cerival/'  # Replace with the desired extraction path\n",
        "\n",
        "# Create the target directory if it doesn't exist\n",
        "os.makedirs(extracted_folder_path, exist_ok=True)\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder_path)\n",
        "\n",
        "# List the contents of the extracted folder\n",
        "extracted_contents = os.listdir(extracted_folder_path)\n",
        "\n",
        "# Print the list of extracted contents\n",
        "print(\"Extracted contents:\", extracted_contents)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0r3eWktkXme"
      },
      "outputs": [],
      "source": [
        "# Function for Formatting Dataset\n",
        "def FormatDataset(dataset_src, dataset_dest, classes):\n",
        "    # Making a Copy of Dataset\n",
        "    new_cropped_dest = [os.path.join(dataset_dest, cls, 'CROPPED') for cls in classes];\n",
        "    new_complete_dest = [os.path.join(dataset_dest, cls, 'COMPLETE') for cls in classes];\n",
        "    cropped_src = [ dataset_src + \"/im_\" + cls + \"/im_\" + cls + \"/CROPPED\" for cls in classes ];\n",
        "    complete_src = [ dataset_src + \"/im_\" + cls + \"/im_\" + cls for cls in classes ];\n",
        "    for (dest1, dest2) in zip(new_cropped_dest, new_complete_dest):\n",
        "        os.makedirs(dest1);\n",
        "        os.makedirs(dest2);\n",
        "    # Formating Cropped Images\n",
        "    for (src,new_dest) in zip(cropped_src, new_cropped_dest):\n",
        "        for file in os.listdir(src):\n",
        "            filename, file_ext = os.path.splitext(file);\n",
        "            if file_ext == '.bmp':\n",
        "                img_des = os.path.join(new_dest, filename + '.jpg');\n",
        "                img = cv2.imread(os.path.join(src, file));\n",
        "                img = cv2.resize(img, (75, 75));\n",
        "                img = cv2.copyMakeBorder(img, 1, 1, 1, 1, cv2.BORDER_CONSTANT, value=0);\n",
        "                img = cv2.blur(img, (2, 2));\n",
        "                img = image_segmentation(img);\n",
        "                cv2.imwrite(img_des ,img);\n",
        "\n",
        "# Source Location for Dataset\n",
        "src = '/content/cerival';\n",
        "# Destination Location for Dataset\n",
        "dest = './CervicalCancer';\n",
        "# Image Classes\n",
        "classes = [\"Dyskeratotic\",\"Koilocytotic\",\"Metaplastic\",\"Parabasal\",\"Superficial-Intermediate\"];\n",
        "# Formatting Dataset\n",
        "FormatDataset(src, dest, classes);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxYLVwHJZ09V",
        "outputId": "49ffa162-bac8-42d3-a524-dc37c02f4157"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Dyskeratotic': 0, 'Koilocytotic': 0, 'Metaplastic': 0, 'Parabasal': 0, 'Superficial-Intermediate': 0}\n"
          ]
        }
      ],
      "source": [
        "root_dir = \"./CervicalCancer\"\n",
        "classes = [\"Dyskeratotic\",\"Koilocytotic\",\"Metaplastic\",\"Parabasal\",\"Superficial-Intermediate\"]\n",
        "\n",
        "def GetDatasetSize(path, classes, main = \"CROPPED\"):\n",
        "    num_of_image = {}\n",
        "    for cls in classes:\n",
        "        # Counting the Number of Files in the Folder\n",
        "        num_of_image[cls] = len(os.listdir(os.path.join(path, cls, main)));\n",
        "    return num_of_image;\n",
        "\n",
        "print(GetDatasetSize(root_dir, classes, \"COMPLETE\"));"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Suv9dPKGn1iH",
        "outputId": "b735e76b-9eec-47c8-8fa1-c9144495913b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dyskeratotic :\n",
            "Total images:  813\n",
            "Training:  569\n",
            "Validation:  122\n",
            "Testing:  122\n",
            "\n",
            "Koilocytotic :\n",
            "Total images:  825\n",
            "Training:  577\n",
            "Validation:  124\n",
            "Testing:  124\n",
            "\n",
            "Metaplastic :\n",
            "Total images:  793\n",
            "Training:  555\n",
            "Validation:  119\n",
            "Testing:  119\n",
            "\n",
            "Parabasal :\n",
            "Total images:  787\n",
            "Training:  550\n",
            "Validation:  118\n",
            "Testing:  119\n",
            "\n",
            "Superficial-Intermediate :\n",
            "Total images:  831\n",
            "Training:  581\n",
            "Validation:  125\n",
            "Testing:  125\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def TrainValTestSplit(root_dir, classes_dir, main = \"CROPPED\", val_ratio = 0.15, test_ratio = 0.15):\n",
        "    for cls in classes_dir:\n",
        "        # Creating Split Folders\n",
        "        os.makedirs('train/' + cls)\n",
        "        os.makedirs('val/' + cls)\n",
        "        os.makedirs('test/' + cls)\n",
        "\n",
        "        # Folder to copy images from\n",
        "        src = os.path.join(root_dir, cls, main);\n",
        "\n",
        "        # Spliting the Files in the Given ratio\n",
        "        allFileNames = os.listdir(src)\n",
        "        np.random.shuffle(allFileNames)\n",
        "        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames), [int(len(allFileNames)* (1 - (val_ratio + test_ratio))), int(len(allFileNames)* (1 - test_ratio))])\n",
        "\n",
        "        train_FileNames = [src+'/'+ name for name in train_FileNames.tolist()]\n",
        "        val_FileNames = [src+'/' + name for name in val_FileNames.tolist()]\n",
        "        test_FileNames = [src+'/' + name for name in test_FileNames.tolist()]\n",
        "\n",
        "        # Printing the Split Details\n",
        "        print(cls,':')\n",
        "        print('Total images: ', len(allFileNames))\n",
        "        print('Training: ', len(train_FileNames))\n",
        "        print('Validation: ', len(val_FileNames))\n",
        "        print('Testing: ', len(test_FileNames))\n",
        "\n",
        "        # Copy-pasting images\n",
        "        for name in train_FileNames:\n",
        "            shutil.copy(name, 'train/' + cls)\n",
        "\n",
        "        for name in val_FileNames:\n",
        "            shutil.copy(name, 'val/' + cls)\n",
        "\n",
        "        for name in test_FileNames:\n",
        "            shutil.copy(name, 'test/' + cls)\n",
        "        print();\n",
        "\n",
        "\n",
        "# Preforming Train / Validation / Test Split\n",
        "root_dir = \"./CervicalCancer\"               # Dataset Root Folder\n",
        "classes_dir = [\"Dyskeratotic\", \"Koilocytotic\", \"Metaplastic\", \"Parabasal\", \"Superficial-Intermediate\"]   # Classes\n",
        "TrainValTestSplit(root_dir, classes_dir);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vt6d1ON-n3vv"
      },
      "outputs": [],
      "source": [
        "# Importing Keras for Image Classification\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.layers import Dense,Conv2D, Flatten, MaxPool2D, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCNMezmUn86o",
        "outputId": "dab6ab5d-e632-4507-9309-e9f660f1aeb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 73, 73, 32)        896       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 71, 71, 32)        9248      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 35, 35, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 33, 33, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 16, 16, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 14, 14, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 7, 7, 128)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 6272)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               802944    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 5)                 645       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 922597 (3.52 MB)\n",
            "Trainable params: 922597 (3.52 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# CNN Model\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Convolutional Layer with input shape (64,64,3)\n",
        "model.add(Conv2D(filters=32, kernel_size= (3,3), activation= 'relu', input_shape=(75,75,3)) )\n",
        "\n",
        "model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu' ))\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu' ))\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), activation='relu' ))\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=128, activation='relu'))\n",
        "model.add(Dense(units=128, activation='relu'))\n",
        "model.add(Dense(units=5, activation='sigmoid'))\n",
        "\n",
        "#model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.002),\n",
        "              #loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']  )\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNnP99BIoCNS"
      },
      "outputs": [],
      "source": [
        "# Expand the size of dataset with new transformed images from the original dataset using ImageDataGenerator.\n",
        "train_datagen = image.ImageDataGenerator(zoom_range = 0.2, shear_range = 0.2 , rescale = 1./255 , horizontal_flip=True)\n",
        "val_datagen = image.ImageDataGenerator(rescale = 1./255)\n",
        "test_datagen = image.ImageDataGenerator(rescale = 1./255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBrb2flloF9U",
        "outputId": "f12ae8a0-47dc-4437-930e-aaff14bccd94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2832 images belonging to 5 classes.\n"
          ]
        }
      ],
      "source": [
        "# Expand the size of dataset with new transformed images from the original dataset using ImageDataGenerator.\n",
        "train_datagen = image.ImageDataGenerator(zoom_range = 0.2, shear_range = 0.2 , rescale = 1./255 , horizontal_flip=True)\n",
        "val_datagen = image.ImageDataGenerator(rescale = 1./255)\n",
        "test_datagen = image.ImageDataGenerator(rescale = 1./255)\n",
        "train_data = train_datagen.flow_from_directory(directory= \"./train\", target_size=(75, 75), batch_size=100, class_mode = 'categorical')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKFKgS-coJuw",
        "outputId": "77e7828e-4a7a-485b-f9c0-d4445fb8b9d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Dyskeratotic': 0,\n",
              " 'Koilocytotic': 1,\n",
              " 'Metaplastic': 2,\n",
              " 'Parabasal': 3,\n",
              " 'Superficial-Intermediate': 4}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data.class_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XLnWmjZoNfQ",
        "outputId": "0aeedb26-e23f-46b1-b97c-35e76d32d690"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 608 images belonging to 5 classes.\n"
          ]
        }
      ],
      "source": [
        "val_data = val_datagen.flow_from_directory(directory= \"./val\", target_size=(75, 75), batch_size=100, class_mode = 'categorical')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpHeI4fcoPHw",
        "outputId": "871999c0-c3f5-4ed2-a622-76582dfde372"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 609 images belonging to 5 classes.\n"
          ]
        }
      ],
      "source": [
        "test_data = test_datagen.flow_from_directory(directory= \"./test\", target_size=(75, 75), batch_size=100, class_mode = 'categorical')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDxKQQcxoS7x"
      },
      "outputs": [],
      "source": [
        "# Adding Model check point Callback\n",
        "mc = ModelCheckpoint(filepath=\"cervical_cancer_best_model.hdf5\", monitor= 'val_accuracy', verbose= 1, save_best_only= True, mode = 'auto');\n",
        "call_back = [ mc ];"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KS93bU_-oVnP",
        "outputId": "4ad5c82c-0cfc-4a36-c2c8-eed399cdcae4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "28/28 [==============================] - ETA: 0s - loss: 1.1178 - accuracy: 0.5673\n",
            "Epoch 1: val_accuracy improved from -inf to 0.60333, saving model to cervical_cancer_best_model.hdf5\n",
            "28/28 [==============================] - 56s 2s/step - loss: 1.1178 - accuracy: 0.5673 - val_loss: 1.0653 - val_accuracy: 0.6033\n",
            "Epoch 2/30\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.9803 - accuracy: 0.6384\n",
            "Epoch 2: val_accuracy improved from 0.60333 to 0.65500, saving model to cervical_cancer_best_model.hdf5\n",
            "28/28 [==============================] - 52s 2s/step - loss: 0.9803 - accuracy: 0.6384 - val_loss: 0.9283 - val_accuracy: 0.6550\n",
            "Epoch 3/30\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.8028 - accuracy: 0.7130\n",
            "Epoch 3: val_accuracy improved from 0.65500 to 0.73667, saving model to cervical_cancer_best_model.hdf5\n",
            "28/28 [==============================] - 54s 2s/step - loss: 0.8028 - accuracy: 0.7130 - val_loss: 0.7081 - val_accuracy: 0.7367\n",
            "Epoch 4/30\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.7030 - accuracy: 0.7398\n",
            "Epoch 4: val_accuracy improved from 0.73667 to 0.74000, saving model to cervical_cancer_best_model.hdf5\n",
            "28/28 [==============================] - 55s 2s/step - loss: 0.7030 - accuracy: 0.7398 - val_loss: 0.6630 - val_accuracy: 0.7400\n",
            "Epoch 5/30\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.7299 - accuracy: 0.7343\n",
            "Epoch 5: val_accuracy improved from 0.74000 to 0.74833, saving model to cervical_cancer_best_model.hdf5\n",
            "28/28 [==============================] - 55s 2s/step - loss: 0.7299 - accuracy: 0.7343 - val_loss: 0.7031 - val_accuracy: 0.7483\n",
            "Epoch 6/30\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.6405 - accuracy: 0.7683\n",
            "Epoch 6: val_accuracy improved from 0.74833 to 0.79333, saving model to cervical_cancer_best_model.hdf5\n",
            "28/28 [==============================] - 54s 2s/step - loss: 0.6405 - accuracy: 0.7683 - val_loss: 0.5997 - val_accuracy: 0.7933\n",
            "Epoch 7/30\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.6458 - accuracy: 0.7617\n",
            "Epoch 7: val_accuracy did not improve from 0.79333\n",
            "28/28 [==============================] - 56s 2s/step - loss: 0.6458 - accuracy: 0.7617 - val_loss: 0.7074 - val_accuracy: 0.7217\n",
            "Epoch 8/30\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.6066 - accuracy: 0.7745\n",
            "Epoch 8: val_accuracy did not improve from 0.79333\n",
            "28/28 [==============================] - 60s 2s/step - loss: 0.6066 - accuracy: 0.7745 - val_loss: 0.6078 - val_accuracy: 0.7933\n",
            "Epoch 9/30\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.5750 - accuracy: 0.7881\n",
            "Epoch 9: val_accuracy improved from 0.79333 to 0.79500, saving model to cervical_cancer_best_model.hdf5\n",
            "28/28 [==============================] - 52s 2s/step - loss: 0.5750 - accuracy: 0.7881 - val_loss: 0.5527 - val_accuracy: 0.7950\n",
            "Epoch 10/30\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.5093 - accuracy: 0.8078\n",
            "Epoch 10: val_accuracy improved from 0.79500 to 0.81333, saving model to cervical_cancer_best_model.hdf5\n",
            "28/28 [==============================] - 54s 2s/step - loss: 0.5093 - accuracy: 0.8078 - val_loss: 0.5084 - val_accuracy: 0.8133\n",
            "Epoch 11/30\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.4784 - accuracy: 0.8170\n",
            "Epoch 11: val_accuracy did not improve from 0.81333\n",
            "28/28 [==============================] - 52s 2s/step - loss: 0.4784 - accuracy: 0.8170 - val_loss: 0.5653 - val_accuracy: 0.7850\n",
            "Epoch 12/30\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.4472 - accuracy: 0.8375\n",
            "Epoch 12: val_accuracy did not improve from 0.81333\n",
            "28/28 [==============================] - 54s 2s/step - loss: 0.4472 - accuracy: 0.8375 - val_loss: 0.5364 - val_accuracy: 0.8117\n",
            "Epoch 13/30\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.4238 - accuracy: 0.8411\n",
            "Epoch 13: val_accuracy improved from 0.81333 to 0.81833, saving model to cervical_cancer_best_model.hdf5\n",
            "28/28 [==============================] - 55s 2s/step - loss: 0.4238 - accuracy: 0.8411 - val_loss: 0.4781 - val_accuracy: 0.8183\n",
            "Epoch 14/30\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.4060 - accuracy: 0.8485\n",
            "Epoch 14: val_accuracy improved from 0.81833 to 0.84000, saving model to cervical_cancer_best_model.hdf5\n",
            "28/28 [==============================] - 54s 2s/step - loss: 0.4060 - accuracy: 0.8485 - val_loss: 0.4798 - val_accuracy: 0.8400\n",
            "Epoch 15/30\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.3850 - accuracy: 0.8591\n",
            "Epoch 15: val_accuracy improved from 0.84000 to 0.86333, saving model to cervical_cancer_best_model.hdf5\n",
            "28/28 [==============================] - 53s 2s/step - loss: 0.3850 - accuracy: 0.8591 - val_loss: 0.3915 - val_accuracy: 0.8633\n",
            "Epoch 16/30\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.3778 - accuracy: 0.8657\n",
            "Epoch 16: val_accuracy did not improve from 0.86333\n",
            "28/28 [==============================] - 61s 2s/step - loss: 0.3778 - accuracy: 0.8657 - val_loss: 0.3987 - val_accuracy: 0.8583\n",
            "Epoch 17/30\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.3303 - accuracy: 0.8869\n",
            "Epoch 17: val_accuracy did not improve from 0.86333\n",
            "28/28 [==============================] - 53s 2s/step - loss: 0.3303 - accuracy: 0.8869 - val_loss: 0.4199 - val_accuracy: 0.8517\n",
            "Epoch 18/30\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.3503 - accuracy: 0.8719\n",
            "Epoch 18: val_accuracy improved from 0.86333 to 0.86500, saving model to cervical_cancer_best_model.hdf5\n",
            "28/28 [==============================] - 55s 2s/step - loss: 0.3503 - accuracy: 0.8719 - val_loss: 0.4009 - val_accuracy: 0.8650\n",
            "Epoch 19/30\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.3313 - accuracy: 0.8763\n",
            "Epoch 19: val_accuracy did not improve from 0.86500\n",
            "28/28 [==============================] - 54s 2s/step - loss: 0.3313 - accuracy: 0.8763 - val_loss: 0.4559 - val_accuracy: 0.8417\n",
            "Epoch 20/30\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.2904 - accuracy: 0.8971\n",
            "Epoch 20: val_accuracy did not improve from 0.86500\n",
            "28/28 [==============================] - 54s 2s/step - loss: 0.2904 - accuracy: 0.8971 - val_loss: 0.4890 - val_accuracy: 0.8433\n",
            "Epoch 21/30\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.3003 - accuracy: 0.8946\n",
            "Epoch 21: val_accuracy did not improve from 0.86500\n",
            "28/28 [==============================] - 54s 2s/step - loss: 0.3003 - accuracy: 0.8946 - val_loss: 0.3687 - val_accuracy: 0.8650\n",
            "Epoch 22/30\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.2735 - accuracy: 0.9052\n",
            "Epoch 22: val_accuracy improved from 0.86500 to 0.88500, saving model to cervical_cancer_best_model.hdf5\n",
            "28/28 [==============================] - 54s 2s/step - loss: 0.2735 - accuracy: 0.9052 - val_loss: 0.3737 - val_accuracy: 0.8850\n",
            "Epoch 23/30\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.2661 - accuracy: 0.9078\n",
            "Epoch 23: val_accuracy did not improve from 0.88500\n",
            "28/28 [==============================] - 54s 2s/step - loss: 0.2661 - accuracy: 0.9078 - val_loss: 0.4469 - val_accuracy: 0.8467\n",
            "Epoch 24/30\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.2760 - accuracy: 0.9026\n",
            "Epoch 24: val_accuracy did not improve from 0.88500\n",
            "28/28 [==============================] - 61s 2s/step - loss: 0.2760 - accuracy: 0.9026 - val_loss: 0.3516 - val_accuracy: 0.8700\n",
            "Epoch 25/30\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.2405 - accuracy: 0.9158\n",
            "Epoch 25: val_accuracy did not improve from 0.88500\n",
            "28/28 [==============================] - 54s 2s/step - loss: 0.2405 - accuracy: 0.9158 - val_loss: 0.3873 - val_accuracy: 0.8733\n",
            "Epoch 26/30\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.2575 - accuracy: 0.9111\n",
            "Epoch 26: val_accuracy improved from 0.88500 to 0.89833, saving model to cervical_cancer_best_model.hdf5\n",
            "28/28 [==============================] - 53s 2s/step - loss: 0.2575 - accuracy: 0.9111 - val_loss: 0.3264 - val_accuracy: 0.8983\n",
            "Epoch 27/30\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.2223 - accuracy: 0.9209\n",
            "Epoch 27: val_accuracy did not improve from 0.89833\n",
            "28/28 [==============================] - 54s 2s/step - loss: 0.2223 - accuracy: 0.9209 - val_loss: 0.4226 - val_accuracy: 0.8600\n",
            "Epoch 28/30\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.2481 - accuracy: 0.9195\n",
            "Epoch 28: val_accuracy did not improve from 0.89833\n",
            "28/28 [==============================] - 53s 2s/step - loss: 0.2481 - accuracy: 0.9195 - val_loss: 0.3517 - val_accuracy: 0.8767\n",
            "Epoch 29/30\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.2378 - accuracy: 0.9136\n",
            "Epoch 29: val_accuracy did not improve from 0.89833\n",
            "28/28 [==============================] - 55s 2s/step - loss: 0.2378 - accuracy: 0.9136 - val_loss: 0.3704 - val_accuracy: 0.8733\n",
            "Epoch 30/30\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1889 - accuracy: 0.9330\n",
            "Epoch 30: val_accuracy did not improve from 0.89833\n",
            "28/28 [==============================] - 54s 2s/step - loss: 0.1889 - accuracy: 0.9330 - val_loss: 0.2894 - val_accuracy: 0.8950\n"
          ]
        }
      ],
      "source": [
        "# Fitting the Model\n",
        "cnn = model.fit(\n",
        "    train_data,\n",
        "    steps_per_epoch = train_data.samples//train_data.batch_size,\n",
        "    epochs = 30,\n",
        "    validation_data = val_data,\n",
        "    validation_steps = val_data.samples//val_data.batch_size,\n",
        "    callbacks = call_back\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7XI4-MYw61N"
      },
      "outputs": [],
      "source": [
        "model.save('cerival_cnn.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5V2vbzDtxC6H",
        "outputId": "fc95b5e3-ae60-4560-f7c9-27c95e73dfb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 25ms/step\n",
            "[[0.48285806 0.8294237  0.1254598  0.00318632 0.9941982 ]]\n",
            "Predicted Class Index: 4\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "from PIL import Image\n",
        "\n",
        "# Function to load and preprocess the image\n",
        "def load_and_preprocess_image(image_path):\n",
        "    # Load and preprocess the new image\n",
        "    img = Image.open(image_path)\n",
        "    img = img.resize((75, 75))  # Adjust the target size to match your model's input size\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "    img_array /= 255.0  # Normalize pixel values to the range [0, 1]\n",
        "\n",
        "    return img_array\n",
        "\n",
        "# Example usage:\n",
        "new_image_path = '/content/CervicalCancer/Superficial-Intermediate/CROPPED/001_03.jpg'\n",
        "new_image = load_and_preprocess_image(new_image_path)\n",
        "\n",
        "# Ensure the input shape matches the expected input size of the model\n",
        "if new_image.shape[1:] != (75, 75, 3):\n",
        "    raise ValueError(f\"Expected image data of shape (1, 75, 75, 3), but got {new_image.shape}\")\n",
        "\n",
        "# Assuming you have a loaded model named 'model'\n",
        "predictions = model.predict(new_image)\n",
        "\n",
        "# Process the predictions as needed for your application\n",
        "print(predictions)\n",
        "predicted_class_index = np.argmax(predictions)\n",
        "print(\"Predicted Class Index:\", predicted_class_index)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CELC2ERLyYcx",
        "outputId": "6e4d318c-7911-4963-daae-e87230592349"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Class Index: 0\n"
          ]
        }
      ],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
